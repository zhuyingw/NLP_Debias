(base) [kw3pj@spydur]: cat slurm-22382.out
I ran on: spdr16
Starting at Sun Oct  9 02:43:45 EDT 2022
Setting up conda...
no change     /usr/local/sw/anaconda/anaconda3/condabin/conda
no change     /usr/local/sw/anaconda/anaconda3/bin/conda
no change     /usr/local/sw/anaconda/anaconda3/bin/conda-env
no change     /usr/local/sw/anaconda/anaconda3/bin/activate
no change     /usr/local/sw/anaconda/anaconda3/bin/deactivate
no change     /usr/local/sw/anaconda/anaconda3/etc/profile.d/conda.sh
no change     /usr/local/sw/anaconda/anaconda3/etc/fish/conf.d/conda.fish
no change     /usr/local/sw/anaconda/anaconda3/shell/condabin/Conda.psm1
no change     /usr/local/sw/anaconda/anaconda3/shell/condabin/conda-hook.ps1
no change     /usr/local/sw/anaconda/anaconda3/lib/python3.8/site-packages/xontrib/conda.xsh
no change     /usr/local/sw/anaconda/anaconda3/etc/profile.d/conda.csh
no change     /home/kw3pj/.bashrc
No action taken.
Setting up cuda...
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0
Running train.py
/home/kw3pj/.conda/envs/powertransformers/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2022-10-09 02:43:51.286008: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-09 02:43:51.286690: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.
/home/kw3pj/backedup/powerTransformer-master/utils_dr.py:134: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tok_li = np.array(tok_li)
/home/kw3pj/backedup/powerTransformer-master/utils_dr.py:134: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  tok_li = np.array(tok_li)
reading from csv
40478
Traceback (most recent call last):
  File "/home/kw3pj/backedup/powerTransformer-master/train.py", line 75, in <module>
    train(args.setup)
  File "/home/kw3pj/backedup/powerTransformer-master/train.py", line 34, in train
    model.to(device_dr)
  File "/home/kw3pj/.conda/envs/powertransformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 927, in to
    return self._apply(convert)
  File "/home/kw3pj/.conda/envs/powertransformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/kw3pj/.conda/envs/powertransformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/kw3pj/.conda/envs/powertransformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 602, in _apply
    param_applied = fn(param)
  File "/home/kw3pj/.conda/envs/powertransformers/lib/python3.10/site-packages/torch/nn/modules/module.py", line 925, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Finished at Sun Oct  9 02:46:42 EDT 2022
